import argparse
import os
import time
import numpy as np
import torch
import matplotlib.pyplot as plt
import matplotlib
from matplotlib import rcParams

from hppo.hppo_actionmask import *
from hppo.hppo_utils import *
from util.configuration import *
from util.util import *
from env.drone_env import DroneEnv
from config import TrainingConfig
from hppo.hppo_actionmask import PPO_Hybrid


def setup_chinese_fonts():
    """è®¾ç½®matplotlibä¸­æ–‡å­—ä½“æ”¯æŒ"""
    try:
        # å°è¯•å¸¸è§çš„ä¸­æ–‡å­—ä½“
        chinese_fonts = [
            'SimHei',  # é»‘ä½“ (Windows)
            'Microsoft YaHei',  # å¾®è½¯é›…é»‘ (Windows)
            'DejaVu Sans',  # DejaVu (è·¨å¹³å°)
            'WenQuanYi Micro Hei',  # æ–‡æ³‰é©¿å¾®ç±³é»‘ (Linux)
            'Noto Sans CJK SC',  # æ€æºé»‘ä½“ (è·¨å¹³å°)
            'PingFang SC',  # è‹¹æ–¹ (macOS)
            'Heiti SC',  # é»‘ä½“ (macOS)
            'STHeiti',  # åæ–‡é»‘ä½“ (macOS)
        ]

        # å°è¯•è®¾ç½®ä¸­æ–‡å­—ä½“
        font_set = False
        for font_name in chinese_fonts:
            try:
                rcParams['font.sans-serif'] = [font_name] + rcParams['font.sans-serif']
                rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜

                # æµ‹è¯•å­—ä½“æ˜¯å¦å¯ç”¨
                fig, ax = plt.subplots(figsize=(1, 1))
                ax.text(0.5, 0.5, 'æµ‹è¯•', fontsize=10)
                plt.close(fig)

                print(f"âœ“ ä¸­æ–‡å­—ä½“è®¾ç½®æˆåŠŸ: {font_name}")
                font_set = True
                break
            except Exception:
                continue

        if not font_set:
            print("âš  è­¦å‘Š: æœªæ‰¾åˆ°åˆé€‚çš„ä¸­æ–‡å­—ä½“ï¼Œå°†ä½¿ç”¨è‹±æ–‡æ ‡ç­¾")
            return False

        return True

    except Exception as e:
        print(f"âš  å­—ä½“è®¾ç½®å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨è‹±æ–‡æ ‡ç­¾")
        return False


@timer
class Trainer(object):
    """
    ä¼˜åŒ–çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒå™¨
    æ”¯æŒé…ç½®æ–‡ä»¶é©±åŠ¨çš„è®­ç»ƒå’Œæ”¹è¿›çš„ç›‘æ§ç³»ç»Ÿ
    """

    def __init__(self, config=None):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨

        Args:
            config: TrainingConfigå®ä¾‹æˆ–è®­ç»ƒæ¨¡å¼å­—ç¬¦ä¸²
        """
        if isinstance(config, str):
            self.config = TrainingConfig(config)
        elif isinstance(config, TrainingConfig):
            self.config = config
        elif config is None:
            self.config = TrainingConfig('standard')
        else:
            # å…¼å®¹æ—§ç‰ˆå‚æ•°æ ¼å¼
            self.config = self._convert_old_args(config)

        # è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
        self.chinese_font_available = setup_chinese_fonts()

        # ä»é…ç½®ä¸­åŠ è½½å‚æ•°
        self._load_config_params()

        # åˆå§‹åŒ–ç¯å¢ƒ
        self.env = DroneEnv(self.config.env_config)
        self.machine_qty = self.env.num_sensors
        self.obs_dim = self.env.observation_space.shape[0]
        self.action_dis_dim = 1
        self.action_con_dim = 2
        self.action_len = 2

        # è®­ç»ƒå†å²è®°å½•
        self.history = {}
        self.total_rewards = []
        self.training_metrics = {
            'episode_rewards': [],
            'episode_lengths': [],
            'data_completion_rates': [],
            'avg_uncertainties': [],
            'communication_counts': [],
            'localization_counts': []
        }

        # åˆ›å»ºä¿å­˜ç›®å½•
        self._setup_save_directory()

        # æ‰“å°é…ç½®ä¿¡æ¯
        self.config.print_config()

    def _load_config_params(self):
        """ä»é…ç½®ä¸­åŠ è½½è®­ç»ƒå‚æ•°"""
        self.device = self.config.device
        self.max_episodes = self.config.max_episodes
        self.buffer_size = self.config.buffer_size
        self.batch_size = self.config.batch_size
        self.agent_save_freq = self.config.agent_save_freq
        self.agent_update_freq = self.config.agent_update_freq
        self.experiment_name = self.config.experiment_name

        # å­¦ä¹ å™¨è¶…å‚æ•°
        self.mid_dim = self.config.mid_dim
        self.lr_actor = self.config.lr_actor
        self.lr_critic = self.config.lr_critic
        self.lr_std = self.config.lr_std
        self.lr_decay_rate = self.config.lr_decay_rate
        self.target_kl_dis = self.config.target_kl_dis
        self.target_kl_con = self.config.target_kl_con
        self.gamma = self.config.gamma
        self.lam = self.config.lam
        self.epochs_update = self.config.epochs_update
        self.v_iters = self.config.v_iters
        self.eps_clip = self.config.eps_clip
        self.max_norm_grad = self.config.max_norm_grad
        self.init_log_std = self.config.init_log_std
        self.coeff_dist_entropy = self.config.coeff_dist_entropy
        self.random_seed = self.config.random_seed
        self.if_use_active_selection = self.config.if_use_active_selection

    def _setup_save_directory(self):
        """è®¾ç½®ä¿å­˜ç›®å½•"""
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        self.save_path = f'log/{self.config.mode}_{timestamp}/'
        if not os.path.exists(self.save_path):
            os.makedirs(self.save_path)

        # ä¿å­˜é…ç½®ä¿¡æ¯
        config_info = {
            'mode': self.config.mode,
            'timestamp': timestamp,
            'device': str(self.device),
            'max_episodes': self.max_episodes,
            'env_config': self.config.env_config
        }
        np.save(self.save_path + 'training_config.npy', config_info)

    def _convert_old_args(self, args):
        """å…¼å®¹æ—§ç‰ˆå‚æ•°æ ¼å¼çš„è½¬æ¢å™¨"""
        config = TrainingConfig('standard')

        # ä»argsä¸­æå–å‚æ•°å¹¶è®¾ç½®åˆ°config
        if hasattr(args, 'max_episodes'):
            config.max_episodes = args.max_episodes
        if hasattr(args, 'lr_actor'):
            config.lr_actor = args.lr_actor
        if hasattr(args, 'experiment_name'):
            config.experiment_name = args.experiment_name

        return config

    def push_history_dis(self, obs, action_mask, act_dis, logp_act_dis, val):
        self.history = {
            'obs': obs,
            'action_mask': action_mask,
            'act_dis': act_dis,
            'logp_act_dis': logp_act_dis,
            'val': val
        }

    def push_history_hybrid(self, obs, action_mask, act_dis, act_con, logp_act_dis, logp_act_con, val):
        self.history = {
            'obs': obs,
            'action_mask': action_mask,
            'act_dis': act_dis,
            'act_con': act_con,
            'logp_act_dis': logp_act_dis,
            'logp_act_con': logp_act_con,
            'val': val
        }

    def unbatchify(self, value_action_logp):
        """
        è§£ææ™ºèƒ½ä½“é€‰æ‹©åŠ¨ä½œçš„è¿”å›å€¼

        Args:
            value_action_logp: PPO_Hybrid.select_actionçš„è¿”å›å€¼ (state_value, actions, logp_actions)

        Returns:
            state_value, actions, logp_actions
        """
        if isinstance(value_action_logp, (tuple, list)) and len(value_action_logp) == 3:
            # å¤„ç†PPO_Hybridè¿”å›çš„ä¸‰å…ƒç»„æ ¼å¼
            state_value = value_action_logp[0]
            actions = value_action_logp[1]  # (action_dis, action_con)
            logp_actions = value_action_logp[2]  # (logp_dis, logp_con)
            return state_value, actions, logp_actions
        elif isinstance(value_action_logp, dict):
            # å¤„ç†å­—å…¸æ ¼å¼ï¼ˆå‘åå…¼å®¹ï¼‰
            state_value = value_action_logp[0]
            actions = value_action_logp[1]
            logp_actions = value_action_logp[2]
            return state_value, actions, logp_actions
        else:
            raise ValueError(f"æ— æ³•è§£ævalue_action_logpçš„æ ¼å¼: {type(value_action_logp)}")

    def initialize_agents(self, random_seed):
        """åˆå§‹åŒ–æ™ºèƒ½ä½“"""
        return PPO_Hybrid(
            self.obs_dim, self.action_dis_dim, self.action_len, self.action_con_dim,
            self.mid_dim, self.lr_actor, self.lr_critic, self.lr_decay_rate,
            self.buffer_size, self.target_kl_dis, self.target_kl_con,
            self.gamma, self.lam, self.epochs_update, self.v_iters,
            self.eps_clip, self.max_norm_grad, self.coeff_dist_entropy,
            random_seed, self.device, self.lr_std, self.init_log_std,
            self.if_use_active_selection
        )

    def train(self, worker_idx=1):
        """
        æ‰§è¡Œè®­ç»ƒå¾ªç¯

        Args:
            worker_idx: å·¥ä½œå™¨ç´¢å¼•
        """
        print(f"\nå¼€å§‹è®­ç»ƒ - å·¥ä½œå™¨ {worker_idx}")

        agent = self.initialize_agents(worker_idx)
        norm_mean = np.zeros(self.obs_dim)
        norm_std = np.ones(self.obs_dim)

        i_episode = 0
        start_time = time.time()

        try:
            while i_episode < self.max_episodes:
                episode_start = time.time()

                # æ”¶é›†ä¸€ä¸ªepisode
                with torch.no_grad():
                    episode_data = self._collect_episode(agent, norm_mean, norm_std, i_episode)

                # è®°å½•è®­ç»ƒæŒ‡æ ‡
                self._record_episode_metrics(episode_data, i_episode)

                # å®šæœŸæ›´æ–°æ™ºèƒ½ä½“
                if i_episode % self.agent_update_freq == 0 and i_episode > 0:
                    norm_mean = agent.buffer.filter()[0]
                    norm_std = agent.buffer.filter()[1]
                    if i_episode > self.agent_save_freq:
                        agent.update(self.batch_size)
                    agent.buffer.clear()

                # å®šæœŸä¿å­˜å’Œå¯è§†åŒ–
                if i_episode % self.agent_save_freq == 0 and i_episode > 0:
                    self._save_checkpoint(i_episode)
                    self._visualize_progress(i_episode)

                i_episode += 1

                # ä¼°ç®—å‰©ä½™æ—¶é—´
                if i_episode % 50 == 0:
                    elapsed = time.time() - start_time
                    avg_time_per_episode = elapsed / i_episode
                    remaining_episodes = self.max_episodes - i_episode
                    eta = remaining_episodes * avg_time_per_episode

                    # GPUä¼˜åŒ–æ¨¡å¼ä¸‹å‡å°‘è¯¦ç»†è¾“å‡º
                    if self.config.mode == 'gpu_optimized':
                        # GPUæ¨¡å¼ä¸‹æ›´ç®€æ´çš„è¿›åº¦æŠ¥å‘Š
                        progress_percent = i_episode / self.max_episodes * 100
                        recent_avg = np.mean(self.total_rewards[-10:]) if len(self.total_rewards) >= 10 else 0
                        print(
                            f"è¿›åº¦: {i_episode}/{self.max_episodes} ({progress_percent:.1f}%) | è¿‘æœŸå¹³å‡å¥–åŠ±: {recent_avg:.2f} | ETA: {eta / 60:.1f}åˆ†é’Ÿ")
                    else:
                        print(
                            f"è®­ç»ƒè¿›åº¦: {i_episode}/{self.max_episodes} ({i_episode / self.max_episodes * 100:.1f}%), é¢„è®¡å‰©ä½™æ—¶é—´: {eta / 60:.1f}åˆ†é’Ÿ")

        except KeyboardInterrupt:
            print(f"\nè®­ç»ƒè¢«ä¸­æ–­ï¼Œå·²å®Œæˆ {i_episode} ä¸ªepisode")
        except Exception as e:
            print(f"\nè®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            raise
        finally:
            # ä¿å­˜æœ€ç»ˆç»“æœ
            self._save_final_results()
            print(f"\nè®­ç»ƒå®Œæˆ! æ€»ç”¨æ—¶: {(time.time() - start_time) / 60:.1f}åˆ†é’Ÿ")

    def _collect_episode(self, agent, norm_mean, norm_std, episode_num):
        """æ”¶é›†ä¸€ä¸ªepisodeçš„æ•°æ®"""
        state, info = self.env.reset()
        action_mask = self.env.action_mask

        total_reward = 0
        step_count = 0
        episode_data = {
            'reward': 0,
            'steps': 0,
            'metrics': None,
            'success': False
        }

        while True:
            # çŠ¶æ€å½’ä¸€åŒ–
            observations_norm = (state - norm_mean) / np.maximum(norm_std, 1e-6)

            # é€‰æ‹©åŠ¨ä½œ
            value_action_logp = agent.select_action(observations_norm, action_mask)
            values, actions, logp_actions = self.unbatchify(value_action_logp)

            # æ„å»ºç¯å¢ƒåŠ¨ä½œ
            action_dict = {
                'discrete': actions[0],
                'continuous': actions[1]
            }

            next_state, reward, done, truncated, info = self.env.step(action_dict)

            # å­˜å‚¨ç»éªŒ
            action_mask_flat = action_mask['discrete_mask']
            self.push_history_hybrid(state, action_mask_flat, actions[0], actions[1],
                                     logp_actions[0], logp_actions[1], values)
            agent.buffer.store_hybrid(
                self.history['obs'], self.history['action_mask'],
                self.history['act_dis'], self.history['act_con'],
                reward, self.history['val'],
                self.history['logp_act_dis'], self.history['logp_act_con']
            )

            total_reward += reward
            step_count += 1
            state = next_state
            action_mask = self.env.action_mask

            if done or truncated:
                episode_data['reward'] = total_reward
                episode_data['steps'] = step_count
                episode_data['metrics'] = self.env.get_performance_metrics()
                episode_data['success'] = done and not truncated

                agent.buffer.finish_path(0)
                break

        return episode_data

    def _record_episode_metrics(self, episode_data, episode_num):
        """è®°å½•episodeæŒ‡æ ‡"""
        self.total_rewards.append(episode_data['reward'])

        # è¯¦ç»†æŒ‡æ ‡è®°å½•
        metrics = episode_data['metrics']
        if metrics:
            self.training_metrics['episode_rewards'].append(episode_data['reward'])
            self.training_metrics['episode_lengths'].append(episode_data['steps'])
            self.training_metrics['data_completion_rates'].append(metrics['data_completion_rate'])
            self.training_metrics['avg_uncertainties'].append(metrics['average_uncertainty'])
            self.training_metrics['communication_counts'].append(metrics['communication_count'])
            self.training_metrics['localization_counts'].append(metrics['localization_count'])

        # è·å–å¥–åŠ±ç»„ä»¶ç»Ÿè®¡
        reward_stats = self.env.get_reward_statistics()

        # GPUä¼˜åŒ–æ¨¡å¼ä¸‹å‡å°‘æ‰“å°é¢‘ç‡
        print_interval = 100 if self.config.mode == 'gpu_optimized' else 10  # GPUæ¨¡å¼ä¸‹æ”¹ä¸º100è½®æ‰“å°ä¸€æ¬¡

        # å®šæœŸæ‰“å°è¯¦ç»†è¿›åº¦
        if episode_num % print_interval == 0:
            recent_rewards = self.total_rewards[-10:] if len(self.total_rewards) >= 10 else self.total_rewards
            avg_reward = np.mean(recent_rewards)

            # å¥–åŠ±åˆ†æ
            reward_trend = self._analyze_reward_trend()

            print(
                f"Episode {episode_num}: å¥–åŠ±={episode_data['reward']:.2f}, æ­¥æ•°={episode_data['steps']}, 10è½®å¹³å‡={avg_reward:.2f}")

            if metrics:
                print(
                    f"  æ•°æ®å®Œæˆç‡: {metrics['data_completion_rate']:.2%}, å¹³å‡ä¸ç¡®å®šæ€§: {metrics['average_uncertainty']:.1f}m")

            # GPUä¼˜åŒ–æ¨¡å¼ä¸‹å‡å°‘è¯¦ç»†è¾“å‡º
            if self.config.mode != 'gpu_optimized':
                # æ‰“å°å¥–åŠ±ç»„ä»¶åˆ†æ
                if reward_stats:
                    print(f"  å¥–åŠ±ç»„ä»¶ - åŸºçº¿:{reward_stats['recent_average_baseline']:.3f}, "
                          f"åŠ¨ä½œ:{reward_stats['recent_average_action']:.3f}, "
                          f"è¿›åº¦:{reward_stats['recent_average_progress']:.3f}")
                    print(f"  å­¦ä¹ é˜¶æ®µ: {reward_stats['current_learning_stage']}, "
                          f"æ•´ä½“è¿›åº¦: {reward_stats['overall_progress']:.1%}")
                    print(f"  æŠ€èƒ½ç­‰çº§ - é€šä¿¡:{reward_stats['communication_skill']:.3f}, "
                          f"å®šä½:{reward_stats['localization_skill']:.3f}")
                    print(f"  æˆåŠŸç‡ - é€šä¿¡:{reward_stats['communication_success_rate']:.1%}, "
                          f"å®šä½:{reward_stats['localization_success_rate']:.1%}")
                    print(f"  æ”¶æ•›è¶‹åŠ¿: {reward_trend}")
            else:
                # GPUä¼˜åŒ–æ¨¡å¼ä¸‹çš„ç®€åŒ–è¾“å‡º
                if reward_stats:
                    print(
                        f"  å­¦ä¹ é˜¶æ®µ: {reward_stats['current_learning_stage']}, è¿›åº¦: {reward_stats['overall_progress']:.1%}, è¶‹åŠ¿: {reward_trend}")

        # æ£€æŸ¥å¥–åŠ±æ”¶æ•›æ€§ï¼ˆGPUä¼˜åŒ–æ¨¡å¼ä¸‹å‡å°‘é¢‘ç‡ï¼‰
        convergence_check_interval = 200 if self.config.mode == 'gpu_optimized' else 50  # GPUæ¨¡å¼ä¸‹æ”¹ä¸º200è½®æ£€æŸ¥ä¸€æ¬¡
        if episode_num % convergence_check_interval == 0 and episode_num > 100:
            self._check_convergence_status(episode_num)

    def _analyze_reward_trend(self, window_size=20):
        """åˆ†æå¥–åŠ±è¶‹åŠ¿"""
        if len(self.total_rewards) < window_size:
            return "æ•°æ®ä¸è¶³"

        recent_rewards = self.total_rewards[-window_size:]

        # è®¡ç®—çº¿æ€§è¶‹åŠ¿
        x = np.arange(len(recent_rewards))
        slope, _ = np.polyfit(x, recent_rewards, 1)

        if slope > 0.01:
            return "ğŸ“ˆ ä¸Šå‡è¶‹åŠ¿"
        elif slope < -0.01:
            return "ğŸ“‰ ä¸‹é™è¶‹åŠ¿"
        else:
            return "â¡ï¸ ç¨³å®š"

    def _check_convergence_status(self, episode_num):
        """æ£€æŸ¥å¥–åŠ±æ”¶æ•›çŠ¶æ€å¹¶æä¾›ç¨³å®šæ€§åˆ†æ"""
        if len(self.total_rewards) < 100:
            return

        # åˆ†ææœ€è¿‘50è½®å’Œä¹‹å‰50è½®çš„å¥–åŠ±
        recent_50 = self.total_rewards[-50:]
        previous_50 = self.total_rewards[-100:-50]

        recent_mean = np.mean(recent_50)
        previous_mean = np.mean(previous_50)
        recent_std = np.std(recent_50)
        previous_std = np.std(previous_50)

        improvement = recent_mean - previous_mean
        stability_change = recent_std - previous_std

        print(f"\nğŸ” æ”¶æ•›æ€§åˆ†æ (Episode {episode_num}):")
        print(f"  æœ€è¿‘50è½®å¹³å‡å¥–åŠ±: {recent_mean:.3f} (æ ‡å‡†å·®: {recent_std:.3f})")
        print(f"  ç›¸æ¯”å‰50è½®æ”¹å–„: {improvement:.3f}")
        print(f"  ç¨³å®šæ€§å˜åŒ–: {stability_change:.3f} ({'æ¶åŒ–' if stability_change > 0 else 'æ”¹å–„'})")

        # ç¨³å®šæ€§è¯„ä¼°
        if recent_std > 1.5:
            print("  âš ï¸ å¥–åŠ±æ³¢åŠ¨å¾ˆå¤§ï¼Œå»ºè®®é™ä½å­¦ä¹ ç‡æˆ–å¢åŠ å¹³æ»‘")
            self._provide_stability_suggestions(recent_std, recent_mean)
        elif recent_std > 0.8:
            print("  ğŸ“Š å¥–åŠ±æ³¢åŠ¨é€‚ä¸­ï¼Œå¯èƒ½éœ€è¦å¾®è°ƒå‚æ•°")
        elif recent_std < 0.3:
            print("  âœ… å¥–åŠ±éå¸¸ç¨³å®š")
        else:
            print("  ğŸ”„ å¥–åŠ±ç¨³å®šæ€§è‰¯å¥½")

        # æ”¶æ•›åˆ¤æ–­
        if improvement > 0.1 and recent_std < 0.5:
            print("  âœ… å¥–åŠ±æ­£åœ¨æ”¶æ•›ä¸”ç¨³å®š")
        elif improvement > 0.05:
            print("  ğŸ“ˆ å¥–åŠ±æŒç»­æ”¹å–„")
        elif abs(improvement) < 0.02 and recent_std < 0.4:
            print("  ğŸ¯ å¯èƒ½å·²è¾¾åˆ°æ”¶æ•›æ¡ä»¶")
        elif recent_std > 1.0:
            print("  âš ï¸ å¥–åŠ±æ³¢åŠ¨è¿‡å¤§ï¼Œå»ºè®®è°ƒæ•´è®­ç»ƒå‚æ•°")
        else:
            print("  ğŸ”„ è®­ç»ƒè¿›è¡Œä¸­ï¼Œéœ€è¦æ›´å¤šæ•°æ®")

        # æ£€æŸ¥å¥–åŠ±è¶‹åŠ¿
        if len(self.total_rewards) >= 200:
            self._analyze_long_term_trend(episode_num)

    def _provide_stability_suggestions(self, std_dev, mean_reward):
        """æä¾›ç¨³å®šæ€§æ”¹å–„å»ºè®®"""
        print(f"\nğŸ’¡ ç¨³å®šæ€§æ”¹å–„å»ºè®®:")

        if std_dev > 2.0:
            print("  ğŸ”§ å»ºè®®å¤§å¹…é™ä½å­¦ä¹ ç‡ (å‡å°‘åˆ°å½“å‰çš„ 0.5å€)")
            print("  ğŸ“Š è€ƒè™‘å¢åŠ æ‰¹å¤„ç†å¤§å°")
            print("  ğŸ¯ ä½¿ç”¨æ›´ä¸¥æ ¼çš„æ¢¯åº¦è£å‰ª")
        elif std_dev > 1.5:
            print("  ğŸ”§ å»ºè®®é€‚åº¦é™ä½å­¦ä¹ ç‡ (å‡å°‘åˆ°å½“å‰çš„ 0.7å€)")
            print("  ğŸ“ˆ è€ƒè™‘å‡å°‘epochs_update")
            print("  ğŸ›ï¸ è°ƒæ•´eps_clipå‚æ•° (å»ºè®®0.05-0.08)")

        if mean_reward < 0:
            print("  ğŸ“‰ å¹³å‡å¥–åŠ±ä¸ºè´Ÿï¼Œå¯èƒ½éœ€è¦:")
            print("     - æ£€æŸ¥å¥–åŠ±å‡½æ•°è®¾è®¡")
            print("     - é™ä½æ¢ç´¢å¼ºåº¦")
            print("     - å¢åŠ åŸºçº¿å¥–åŠ±")
        elif mean_reward > 3.0:
            print("  ğŸ“ˆ å¥–åŠ±è¾ƒé«˜ä½†ä¸ç¨³å®šï¼Œå»ºè®®:")
            print("     - ä½¿ç”¨æ›´ä¿å®ˆçš„æ›´æ–°ç­–ç•¥")
            print("     - å¢åŠ å¥–åŠ±å¹³æ»‘")

        print("  ğŸ”„ ä¹Ÿå¯ä»¥å°è¯•ä½¿ç”¨ 'stable_gpu' é…ç½®æ¨¡å¼")

    def _analyze_long_term_trend(self, episode_num):
        """åˆ†æé•¿æœŸè®­ç»ƒè¶‹åŠ¿"""
        if len(self.total_rewards) < 200:
            return

        # åˆ†æˆ4ä¸ªé˜¶æ®µåˆ†æ
        quarter_size = len(self.total_rewards) // 4
        quarters = [
            self.total_rewards[i * quarter_size:(i + 1) * quarter_size]
            for i in range(4)
        ]

        quarter_means = [np.mean(q) for q in quarters]
        quarter_stds = [np.std(q) for q in quarters]

        print(f"\nğŸ“Š é•¿æœŸè¶‹åŠ¿åˆ†æ:")
        print(f"  å„é˜¶æ®µå¹³å‡å¥–åŠ±: {[f'{m:.2f}' for m in quarter_means]}")
        print(f"  å„é˜¶æ®µç¨³å®šæ€§: {[f'{s:.2f}' for s in quarter_stds]}")

        # è¶‹åŠ¿åˆ¤æ–­
        if quarter_means[-1] > quarter_means[0] * 1.1:
            print("  ğŸ“ˆ é•¿æœŸä¸Šå‡è¶‹åŠ¿æ˜æ˜¾")
        elif quarter_means[-1] < quarter_means[0] * 0.9:
            print("  ğŸ“‰ é•¿æœŸä¸‹é™è¶‹åŠ¿ï¼Œéœ€è¦æ£€æŸ¥")
        else:
            print("  â¡ï¸ é•¿æœŸè¶‹åŠ¿å¹³ç¨³")

        # ç¨³å®šæ€§è¶‹åŠ¿
        if quarter_stds[-1] < quarter_stds[0] * 0.8:
            print("  âœ… ç¨³å®šæ€§æ˜¾è‘—æ”¹å–„")
        elif quarter_stds[-1] > quarter_stds[0] * 1.2:
            print("  âš ï¸ ç¨³å®šæ€§æœ‰æ‰€æ¶åŒ–")
        else:
            print("  ğŸ”„ ç¨³å®šæ€§åŸºæœ¬ä¿æŒ")

    def _visualize_progress(self, episode_num):
        """å¯è§†åŒ–è®­ç»ƒè¿›åº¦"""
        # GPUä¼˜åŒ–æ¨¡å¼ä¸‹å®Œå…¨è·³è¿‡è®­ç»ƒè¿‡ç¨‹ä¸­çš„å¯è§†åŒ–ï¼Œåªåœ¨æœ€ç»ˆç”Ÿæˆ
        if self.config.mode == 'gpu_optimized':
            return  # ç›´æ¥è¿”å›ï¼Œä¸ä¿å­˜ä»»ä½•ä¸­é—´å›¾åƒ

        # å…¶ä»–æ¨¡å¼æ­£å¸¸ä¿å­˜ç¯å¢ƒå¯è§†åŒ–
        save_path = self.save_path + f'environment_ep{episode_num}.png'
        self.env.visualize_environment(save_path)

        # ç”Ÿæˆå¥–åŠ±åˆ†æå›¾è¡¨
        if episode_num % 100 == 0 and len(self.total_rewards) > 20:
            self._create_reward_analysis_plot(episode_num)

    def _create_reward_analysis_plot(self, episode_num):
        """åˆ›å»ºå¥–åŠ±åˆ†æå›¾è¡¨"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 8))

        # æ ¹æ®å­—ä½“æ”¯æŒæƒ…å†µé€‰æ‹©æ ‡ç­¾è¯­è¨€
        if self.chinese_font_available:
            labels = {
                'title1': 'å¥–åŠ±æ”¶æ•›æ›²çº¿',
                'title2': 'å¥–åŠ±åˆ†å¸ƒ',
                'title3': 'å¥–åŠ±ç»„ä»¶å˜åŒ–',
                'title4': 'å­¦ä¹ æ›²çº¿ (åˆ†æ®µå¹³å‡)',
                'xlabel1': 'è®­ç»ƒè½®æ¬¡',
                'ylabel1': 'æ€»å¥–åŠ±',
                'xlabel2': 'å¥–åŠ±å€¼',
                'ylabel2': 'é¢‘æ¬¡',
                'xlabel3': 'æœ€è¿‘æ­¥æ•°',
                'ylabel3': 'å¥–åŠ±å€¼',
                'xlabel4': 'è®­ç»ƒé˜¶æ®µ',
                'ylabel4': 'å¹³å‡å¥–åŠ±',
                'original_reward': 'åŸå§‹å¥–åŠ±',
                'moving_avg': 'ç§»åŠ¨å¹³å‡',
                'mean': 'å‡å€¼',
                'baseline_penalty': 'åŸºçº¿æƒ©ç½š',
                'action_reward': 'åŠ¨ä½œå¥–åŠ±',
                'progress_reward': 'è¿›åº¦å¥–åŠ±',
                'trend': 'è¶‹åŠ¿',
                'main_title': f'è®­ç»ƒåˆ†æ - Episode {episode_num}'
            }
        else:
            labels = {
                'title1': 'Reward Convergence',
                'title2': 'Reward Distribution',
                'title3': 'Reward Components',
                'title4': 'Learning Curve',
                'xlabel1': 'Episodes',
                'ylabel1': 'Total Reward',
                'xlabel2': 'Reward Value',
                'ylabel2': 'Frequency',
                'xlabel3': 'Recent Steps',
                'ylabel3': 'Reward Value',
                'xlabel4': 'Training Stage',
                'ylabel4': 'Average Reward',
                'original_reward': 'Original Reward',
                'moving_avg': 'Moving Average',
                'mean': 'Mean',
                'baseline_penalty': 'Baseline Penalty',
                'action_reward': 'Action Reward',
                'progress_reward': 'Progress Reward',
                'trend': 'Trend',
                'main_title': f'Training Analysis - Episode {episode_num}'
            }

        # 1. å¥–åŠ±å†å²å’Œç§»åŠ¨å¹³å‡
        episodes = range(len(self.total_rewards))
        ax1.plot(episodes, self.total_rewards, alpha=0.3, color='lightblue', label=labels['original_reward'])

        if len(self.total_rewards) > 10:
            window = min(20, len(self.total_rewards) // 5)
            moving_avg = np.convolve(self.total_rewards, np.ones(window) / window, mode='valid')
            ax1.plot(range(window - 1, len(self.total_rewards)), moving_avg,
                     color='red', linewidth=2, label=f"{labels['moving_avg']}({window})")

        ax1.set_title(labels['title1'])
        ax1.set_xlabel(labels['xlabel1'])
        ax1.set_ylabel(labels['ylabel1'])
        ax1.legend()
        ax1.grid(True, alpha=0.3)

        # 2. å¥–åŠ±åˆ†å¸ƒç›´æ–¹å›¾
        ax2.hist(self.total_rewards, bins=30, alpha=0.7, color='green', edgecolor='black')
        ax2.axvline(np.mean(self.total_rewards), color='red', linestyle='--',
                    label=f"{labels['mean']}: {np.mean(self.total_rewards):.2f}")
        ax2.set_title(labels['title2'])
        ax2.set_xlabel(labels['xlabel2'])
        ax2.set_ylabel(labels['ylabel2'])
        ax2.legend()
        ax2.grid(True, alpha=0.3)

        # 3. å¥–åŠ±ç»„ä»¶åˆ†æï¼ˆå¦‚æœæœ‰æ•°æ®ï¼‰
        if hasattr(self.env, 'reward_components') and self.env.reward_components:
            components = self.env.reward_components[-min(50, len(self.env.reward_components)):]

            # æ£€æŸ¥ç»„ä»¶æ˜¯å¦åŒ…å«æ–°çš„å­—æ®µå
            if len(components) > 0 and 'baseline_penalty' in components[0]:
                baseline_penalties = [c['baseline_penalty'] for c in components]
                action_rewards = [c['action_reward'] for c in components]
                progress_rewards = [c['progress_reward'] for c in components]

                x = range(len(components))
                ax3.plot(x, baseline_penalties, label=labels['baseline_penalty'], alpha=0.8, color='red')
                ax3.plot(x, action_rewards, label=labels['action_reward'], alpha=0.8, color='blue')
                ax3.plot(x, progress_rewards, label=labels['progress_reward'], alpha=0.8, color='green')

                ax3.set_title(labels['title3'])
                ax3.set_xlabel(labels['xlabel3'])
                ax3.set_ylabel(labels['ylabel3'])
                ax3.legend()
                ax3.grid(True, alpha=0.3)
            else:
                # å¦‚æœæ²¡æœ‰æ–°å­—æ®µï¼Œæ˜¾ç¤ºæç¤ºä¿¡æ¯
                ax3.text(0.5, 0.5, 'å¥–åŠ±ç»„ä»¶æ•°æ®ä¸å¯ç”¨\n(å¯èƒ½ä½¿ç”¨äº†æ—§çš„å¥–åŠ±ç³»ç»Ÿ)',
                         transform=ax3.transAxes, ha='center', va='center', fontsize=12)
                ax3.set_title(labels['title3'])
        else:
            ax3.text(0.5, 0.5, 'æ— å¥–åŠ±ç»„ä»¶æ•°æ®', transform=ax3.transAxes, ha='center', va='center')
            ax3.set_title(labels['title3'])

        # 4. å­¦ä¹ æ›²çº¿ï¼ˆåˆ†æ®µåˆ†æï¼‰
        if len(self.total_rewards) > 50:
            # å°†è®­ç»ƒè¿‡ç¨‹åˆ†ä¸º10æ®µ
            segment_size = max(5, len(self.total_rewards) // 10)
            segments = []
            segment_means = []

            for i in range(0, len(self.total_rewards), segment_size):
                segment = self.total_rewards[i:i + segment_size]
                if len(segment) >= 3:  # è‡³å°‘3ä¸ªæ•°æ®ç‚¹
                    segments.append(i // segment_size)
                    segment_means.append(np.mean(segment))

            ax4.plot(segments, segment_means, marker='o', linewidth=2, markersize=6, color='purple')
            ax4.set_title(labels['title4'])
            ax4.set_xlabel(labels['xlabel4'])
            ax4.set_ylabel(labels['ylabel4'])
            ax4.grid(True, alpha=0.3)

            # æ·»åŠ è¶‹åŠ¿çº¿
            if len(segments) > 2:
                z = np.polyfit(segments, segment_means, 1)
                p = np.poly1d(z)
                ax4.plot(segments, p(segments), "--", alpha=0.8, color='red',
                         label=f"{labels['trend']} ({z[0]:.3f})")
                ax4.legend()

        plt.suptitle(labels['main_title'], fontsize=16)
        plt.tight_layout()
        plt.savefig(self.save_path + f'reward_analysis_ep{episode_num}.png',
                    dpi=150, bbox_inches='tight', facecolor='white')
        plt.close()

        print(f"  ğŸ’¾ å¥–åŠ±åˆ†æå›¾è¡¨å·²ä¿å­˜: reward_analysis_ep{episode_num}.png")

    def _save_checkpoint(self, episode_num):
        """ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹"""
        checkpoint_data = {
            'episode': episode_num,
            'total_rewards': self.total_rewards,
            'training_metrics': self.training_metrics,
            'config': self.config.mode
        }
        np.save(self.save_path + f'checkpoint_ep{episode_num}.npy', checkpoint_data)

    def _save_final_results(self):
        """ä¿å­˜æœ€ç»ˆè®­ç»ƒç»“æœ"""
        # ä¿å­˜è¯¦ç»†çš„è®­ç»ƒå†å²
        final_data = {
            'total_rewards': self.total_rewards,
            'training_metrics': self.training_metrics,
            'config_mode': self.config.mode,
            'final_performance': self.env.get_performance_metrics()
        }
        np.save(self.save_path + 'final_training_results.npy', final_data)

        # ç”Ÿæˆè®­ç»ƒæ€»ç»“
        self._generate_training_summary()

    def _generate_training_summary(self):
        """ç”Ÿæˆè®­ç»ƒæ€»ç»“æŠ¥å‘Š"""
        print(f"\n{'=' * 50}")
        print("è®­ç»ƒæ€»ç»“æŠ¥å‘Š")
        print(f"{'=' * 50}")

        if self.total_rewards:
            print(f"æ€»è®­ç»ƒè½®æ¬¡: {len(self.total_rewards)}")
            print(f"å¹³å‡å¥–åŠ±: {np.mean(self.total_rewards):.3f}")
            print(f"æœ€é«˜å¥–åŠ±: {np.max(self.total_rewards):.3f}")
            print(f"æœ€ä½å¥–åŠ±: {np.min(self.total_rewards):.3f}")
            print(f"å¥–åŠ±æ ‡å‡†å·®: {np.std(self.total_rewards):.3f}")

            if len(self.total_rewards) >= 100:
                last_100 = self.total_rewards[-100:]
                print(f"æœ€å100è½®å¹³å‡å¥–åŠ±: {np.mean(last_100):.3f}")

            if len(self.total_rewards) >= 50:
                first_half = self.total_rewards[:len(self.total_rewards) // 2]
                second_half = self.total_rewards[len(self.total_rewards) // 2:]
                improvement = np.mean(second_half) - np.mean(first_half)
                print(f"è®­ç»ƒæ”¹å–„ç¨‹åº¦: {improvement:.3f}")

        # ç¯å¢ƒæ€§èƒ½æŒ‡æ ‡
        if hasattr(self, 'env'):
            final_metrics = self.env.get_performance_metrics()
            print(f"\næœ€ç»ˆç¯å¢ƒæ€§èƒ½:")
            print(f"æ•°æ®æ”¶é›†å®Œæˆç‡: {final_metrics['data_completion_rate']:.2%}")
            print(f"å¹³å‡å®šä½ä¸ç¡®å®šæ€§: {final_metrics['average_uncertainty']:.2f}m")
            print(f"é€šä¿¡æ¬¡æ•°: {final_metrics['communication_count']}")
            print(f"å®šä½æ¬¡æ•°: {final_metrics['localization_count']}")

    def save_data(self):
        """ä¿å­˜è®­ç»ƒæ•°æ®å¹¶ç”Ÿæˆå¯è§†åŒ–"""
        if not self.total_rewards:
            print("è­¦å‘Š: æ²¡æœ‰è®­ç»ƒæ•°æ®å¯ä¿å­˜")
            return

        # ä¿å­˜åŸºç¡€æ•°æ®
        np.save(self.save_path + 'total_reward_history.npy', self.total_rewards)

        # ç”Ÿæˆç»¼åˆåˆ†æå›¾è¡¨
        self._create_comprehensive_plots()

    def _create_comprehensive_plots(self):
        """åˆ›å»ºç»¼åˆåˆ†æå›¾è¡¨"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))

        # æ ¹æ®å­—ä½“æ”¯æŒæƒ…å†µé€‰æ‹©æ ‡ç­¾è¯­è¨€
        if self.chinese_font_available:
            labels = {
                'title1': 'æ€»å¥–åŠ±å†å²',
                'title2': 'å¥–åŠ±åˆ†å¸ƒ',
                'title3': 'Episodeé•¿åº¦å˜åŒ–',
                'title4': 'å­¦ä¹ æ›²çº¿(åˆ†æ®µå¹³å‡)',
                'xlabel1': 'è®­ç»ƒè½®æ¬¡',
                'ylabel1': 'æ€»å¥–åŠ±',
                'xlabel2': 'å¥–åŠ±å€¼',
                'ylabel2': 'é¢‘æ¬¡',
                'xlabel3': 'è®­ç»ƒè½®æ¬¡',
                'ylabel3': 'æ­¥æ•°',
                'xlabel4': 'è®­ç»ƒé˜¶æ®µ',
                'ylabel4': 'å¹³å‡å¥–åŠ±',
                'moving_avg': 'ç§»åŠ¨å¹³å‡',
                'mean': 'å¹³å‡å€¼'
            }
        else:
            labels = {
                'title1': 'Total Reward History',
                'title2': 'Reward Distribution',
                'title3': 'Episode Length Changes',
                'title4': 'Learning Curve (Segmented)',
                'xlabel1': 'Episodes',
                'ylabel1': 'Total Reward',
                'xlabel2': 'Reward Value',
                'ylabel2': 'Frequency',
                'xlabel3': 'Episodes',
                'ylabel3': 'Steps',
                'xlabel4': 'Training Stage',
                'ylabel4': 'Average Reward',
                'moving_avg': 'Moving Average',
                'mean': 'Mean'
            }

        # 1. æ€»å¥–åŠ±å†å²
        axes[0, 0].plot(self.total_rewards, alpha=0.7, color='blue', linewidth=1)
        if len(self.total_rewards) > 20:
            window_size = min(20, len(self.total_rewards) // 5)
            moving_avg = np.convolve(self.total_rewards, np.ones(window_size) / window_size, mode='valid')
            axes[0, 0].plot(range(window_size - 1, len(self.total_rewards)), moving_avg,
                            color='red', linewidth=2, label=f"{labels['moving_avg']} ({window_size})")
            axes[0, 0].legend()
        axes[0, 0].set_title(labels['title1'])
        axes[0, 0].set_xlabel(labels['xlabel1'])
        axes[0, 0].set_ylabel(labels['ylabel1'])
        axes[0, 0].grid(True, alpha=0.3)

        # 2. å¥–åŠ±åˆ†å¸ƒ
        axes[0, 1].hist(self.total_rewards, bins=30, alpha=0.7, color='green', edgecolor='black')
        axes[0, 1].axvline(np.mean(self.total_rewards), color='red', linestyle='--',
                           label=f"{labels['mean']}: {np.mean(self.total_rewards):.2f}")
        axes[0, 1].set_title(labels['title2'])
        axes[0, 1].set_xlabel(labels['xlabel2'])
        axes[0, 1].set_ylabel(labels['ylabel2'])
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)

        # 3. è®­ç»ƒæŒ‡æ ‡ï¼ˆå¦‚æœæœ‰ï¼‰
        if self.training_metrics['episode_lengths']:
            axes[1, 0].plot(self.training_metrics['episode_lengths'], color='orange', alpha=0.7)
            axes[1, 0].set_title(labels['title3'])
            axes[1, 0].set_xlabel(labels['xlabel3'])
            axes[1, 0].set_ylabel(labels['ylabel3'])
            axes[1, 0].grid(True, alpha=0.3)

        # 4. å­¦ä¹ æ›²çº¿å¯¹æ¯”
        if len(self.total_rewards) > 50:
            segment_size = len(self.total_rewards) // 10
            segments = [self.total_rewards[i:i + segment_size] for i in range(0, len(self.total_rewards), segment_size)]
            segment_means = [np.mean(seg) for seg in segments if seg]

            axes[1, 1].plot(range(len(segment_means)), segment_means, marker='o', linewidth=2, color='purple')
            axes[1, 1].set_title(labels['title4'])
            axes[1, 1].set_xlabel(labels['xlabel4'])
            axes[1, 1].set_ylabel(labels['ylabel4'])
            axes[1, 1].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(self.save_path + 'training_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--device', default=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),
                        help='Device.')
    parser.add_argument('--max_episodes', type=int, default=1001, help='The max episodes per agent per run.')
    parser.add_argument('--buffer_size', type=int, default=6000, help='The maximum size of the PPOBuffer.')
    parser.add_argument('--batch_size', type=int, default=64, help='The sample batch size.')
    parser.add_argument('--agent_save_freq', type=int, default=10, help='The frequency of the agent saving.')
    parser.add_argument('--agent_update_freq', type=int, default=10, help='The frequency of the agent updating.')
    parser.add_argument('--lr_actor', type=float, default=0.0001,
                        help='The learning rate of actor_con.')  # ä»0.0003é™ä¸º0.0001
    parser.add_argument('--lr_actor_param', type=float, default=0.0001,
                        help='The learning rate of critic.')  # ä»0.001é™ä¸º0.0001
    parser.add_argument('--lr_std', type=float, default=0.001, help='The learning rate of log_std.')  # ä»0.004é™ä¸º0.001
    parser.add_argument('--lr_decay_rate', type=float, default=0.998,
                        help='Factor of learning rate decay.')  # ä»0.995æ”¹ä¸º0.998ï¼Œè¡°å‡æ›´æ…¢
    parser.add_argument('--mid_dim', type=list, default=[256, 128, 64], help='The middle dimensions of both nets.')
    parser.add_argument('--gamma', type=float, default=0.99, help='Discounted of future rewards.')
    parser.add_argument('--lam', type=float, default=0.95,  # ä»0.8æ”¹ä¸º0.95ï¼Œæé«˜GAEä¼°è®¡ç¨³å®šæ€§
                        help='Lambda for GAE-Lambda. (Always between 0 and 1, close to 1.)')
    parser.add_argument('--epochs_update', type=int, default=10,  # ä»20é™ä¸º10ï¼Œå‡å°‘æ¯æ¬¡æ›´æ–°çš„epochs
                        help='Maximum number of gradient descent steps to take on policy loss per epoch. (Early stopping may cause optimizer to take fewer than this.)')
    parser.add_argument('--v_iters', type=int, default=1,
                        help='Number of gradient descent steps to take on value function per epoch.')
    parser.add_argument('--target_kl_dis', type=float, default=0.01,  # ä»0.025é™ä¸º0.01ï¼Œæ›´ä¸¥æ ¼çš„KLé™åˆ¶
                        help='Roughly what KL divergence we think is appropriate between new and old policies after an update. This will get used for early stopping. (Usually small, 0.01 or 0.05.)')
    parser.add_argument('--target_kl_con', type=float, default=0.02,  # ä»0.05é™ä¸º0.02ï¼Œæ›´ä¸¥æ ¼çš„KLé™åˆ¶
                        help='Roughly what KL divergence we think is appropriate between new and old policies after an update. This will get used for early stopping. (Usually small, 0.01 or 0.05.)')
    parser.add_argument('--eps_clip', type=float, default=0.1,
                        help='The clip ratio when calculate surr.')  # ä»0.2é™ä¸º0.1ï¼Œæ›´ä¿å®ˆçš„è£å‰ª
    parser.add_argument('--max_norm_grad', type=float, default=2.0,
                        help='max norm of the gradients.')  # ä»5.0é™ä¸º2.0ï¼Œæ›´ä¸¥æ ¼çš„æ¢¯åº¦è£å‰ª
    parser.add_argument('--init_log_std', type=float, default=-1.5,  # ä»-1.0æ”¹ä¸º-1.5ï¼Œåˆå§‹æ¢ç´¢æ›´ä¿å®ˆ
                        help='The initial log_std of Normal in continuous pattern.')
    parser.add_argument('--coeff_dist_entropy', type=float, default=0.01,  # ä»0.005æ”¹ä¸º0.01ï¼Œå¢åŠ æ¢ç´¢
                        help='The coefficient of distribution entropy.')
    parser.add_argument('--random_seed', type=int, default=1, help='The random seed.')
    parser.add_argument('--record_mark', type=str, default='renaissance',
                        help='The mark that differentiates different experiments.')
    parser.add_argument('--if_use_active_selection', type=bool, default=False,
                        help='Whether use active selection in the exploration.')
    parser.add_argument('--experiment_name', type=str, default='optimized_drone',
                        help='The name of the experiment.')  # æ”¹åä»¥åŒºåˆ†ä¼˜åŒ–ç‰ˆæœ¬

    version_no = "RTS-T2-20240507164500"
    mode = "main_train"
    data_source = "pk"
    wetConfig = {"version_no": version_no, "mode": mode, "data_source": data_source}
    parser.add_argument('--wetConfig', type=dict, default=wetConfig, help='wet config')

    args = parser.parse_args()

    # training through multiprocess
    trainer = Trainer(args)
    trainer.train(1)
    trainer.save_data()
